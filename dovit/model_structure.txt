DoVit(
  (module): EncoderDecoder(

    //--------------------------------------------------------------------------
    // Initial Stage: Patch Embedding & Positional Encoding
    //--------------------------------------------------------------------------
    (backbone): VisionTransformer(
      (patch_embed): PatchEmbed(
        (adap_padding): AdaptivePadding()
        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))
      )
      // Implicit: Addition of Class Token (if used for classification, though DoVit here is for segmentation)
      // Implicit: Addition of Positional Embeddings
      (drop_after_pos): Dropout(p=0.0, inplace=False)

      (layers): ModuleList(

        //--------------------------------------------------------------------------
        // Stage 1: Early Backbone Layers (Layers 1-6 of Backbone)
        //--------------------------------------------------------------------------
        (0): TransformerEncoderLayer( // Layer 1
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): Identity() // As per original layer 0
          )
        )
        (1): TransformerEncoderLayer( // Layer 2
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath() // As per original layer 1+
          )
        )
        (2): TransformerEncoderLayer( // Layer 3
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (3): TransformerEncoderLayer( // Layer 4
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (4): TransformerEncoderLayer( // Layer 5
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (5): TransformerEncoderLayer( // Layer 6
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
    // Output of backbone.layers[5] (Layer 6) would feed into aux_head_1
    // End of Stage 1 Backbone Layers (within ModuleList)
    // VisionTransformer backbone definition continues after aux_head_1, aux_head_2, aux_head_3 are defined,
    // then Stage 2 layers [6-11] are added to this same ModuleList, and so on.
    // For clarity in this complete printout, aux_heads will be listed after the full backbone.
    // The conceptual "exit" is after processing this layer.

        //--------------------------------------------------------------------------
        // Stage 2: Mid-Early Backbone Layers (Layers 7-12 of Backbone)
        //--------------------------------------------------------------------------
        (6): TransformerEncoderLayer( // Layer 7
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (7): TransformerEncoderLayer( // Layer 8
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (8): TransformerEncoderLayer( // Layer 9
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (9): TransformerEncoderLayer( // Layer 10
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (10): TransformerEncoderLayer( // Layer 11
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (11): TransformerEncoderLayer( // Layer 12
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
    // Output of backbone.layers[11] (Layer 12) would feed into aux_head_2
    // End of Stage 2 Backbone Layers

        //--------------------------------------------------------------------------
        // Stage 3: Mid-Late Backbone Layers (Layers 13-18 of Backbone)
        //--------------------------------------------------------------------------
        (12): TransformerEncoderLayer( // Layer 13
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (13): TransformerEncoderLayer( // Layer 14
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (14): TransformerEncoderLayer( // Layer 15
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (15): TransformerEncoderLayer( // Layer 16
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (16): TransformerEncoderLayer( // Layer 17
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (17): TransformerEncoderLayer( // Layer 18
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
    // Output of backbone.layers[17] (Layer 18) would feed into aux_head_3
    // End of Stage 3 Backbone Layers

        //--------------------------------------------------------------------------
        // Stage 4: Late Backbone Layers (Layers 19-24 of Backbone) & Final Normalization
        //--------------------------------------------------------------------------
        (18): TransformerEncoderLayer( // Layer 19
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (19): TransformerEncoderLayer( // Layer 20
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (20): TransformerEncoderLayer( // Layer 21
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (21): TransformerEncoderLayer( // Layer 22
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (22): TransformerEncoderLayer( // Layer 23
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
        (23): TransformerEncoderLayer( // Layer 24
          (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
          )
        )
      ) // End of ModuleList for backbone layers
      (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True) // Final LayerNorm for the backbone
    ) // End of VisionTransformer backbone

    //--------------------------------------------------------------------------
    // EXIT POINT 1 (Takes input after backbone.layers[5] - Layer 6)
    //--------------------------------------------------------------------------
    (aux_head_1): AuxiliaryHead(
      (conv_seg): Conv2d(1024, 30, kernel_size=(1, 1), stride=(1, 1))
      (loss_decode): CrossEntropyLoss(avg_non_ignore=False, ignore_index=255)
    )

    //--------------------------------------------------------------------------
    // EXIT POINT 2 (Takes input after backbone.layers[11] - Layer 12)
    //--------------------------------------------------------------------------
    (aux_head_2): AuxiliaryHead(
      (conv_seg): Conv2d(1024, 30, kernel_size=(1, 1), stride=(1, 1))
      (loss_decode): CrossEntropyLoss(avg_non_ignore=False, ignore_index=255)
    )

    //--------------------------------------------------------------------------
    // EXIT POINT 3 (Takes input after backbone.layers[17] - Layer 18)
    //--------------------------------------------------------------------------
    (aux_head_3): AuxiliaryHead(
      (conv_seg): Conv2d(1024, 30, kernel_size=(1, 1), stride=(1, 1))
      (loss_decode): CrossEntropyLoss(avg_non_ignore=False, ignore_index=255)
    )

    //--------------------------------------------------------------------------
    // FINAL EXIT POINT (Takes input after entire Backbone: backbone.layers[23] & backbone.ln1)
    //--------------------------------------------------------------------------
    (decode_head): SegmenterMaskTransformerHead(
      input_transform=None, ignore_index=255, align_corners=False
      (loss_decode): CrossEntropyLoss(avg_non_ignore=False)
      (layers): ModuleList( // These are part of the DECODER head
        (0): TransformerEncoderLayer(
          (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): Identity() // As per original decode_head layer 0
          )
        )
        (1): TransformerEncoderLayer(
          (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
            )
            (proj_drop): Dropout(p=0.0, inplace=False)
            (dropout_layer): Dropout(p=0.0, inplace=False)
          )
          (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (activate): GELU(approximate='none')
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=1024, out_features=4096, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=4096, out_features=1024, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath() // As per original decode_head layer 1
          )
        )
      )
      (dec_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (patch_proj): Linear(in_features=1024, out_features=1024, bias=False)
      (classes_proj): Linear(in_features=1024, out_features=1024, bias=False)
      (decoder_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (mask_norm): LayerNorm((150,), eps=1e-05, elementwise_affine=True) // From original
    )

    init_cfg={'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}
  ) // End of EncoderDecoder
) // End of DoVit